# MAI-7Term-InfoSearch
This repository contains lab assignments for the "Information search" course taught of the 5th term at Moscow Aviation Institute (MAI), Faculty No. 8 (Information Technology and Applied Mathematics).

---
# Описание сервисов
___
Проект развёртывается через Docker Compose и состоит из нескольких контейнеров, которые запускаются последовательно в зависимости от готовности баз данных (через depends_on + healthcheck). Данные и артефакты пайплайна сохраняются в примонтированных томах/папках, поэтому результаты между перезапусками не пропадают.

### 1. Сервис mysql (wiki-mysql)

**Назначение: **реляционная БД (MySQL 8.0) для хранения структурированных данных, необходимых на этапе инициализации.

### 2. Сервис bootstrap (wiki-bootstrap)

**Назначение:** одноразовая инициализация для загрузки и импорта дампов википедии и викихранилища в  MySQL для дальнейшего использования их посиковым роботом.

### 3. Сервис mongo (wiki-mongo)

**Назначение:** MongoDB для хранения документов (HTML/тексты/метаданные), которые формируются осле работы поискового робота и используются далее.

### 4. Сервис crawler (wiki-crawler)

**Назначение:** поисковой робот,, который выполняет сбор данных на основе скачанных дампов википедии и викихранилища  и сохраняет результат в MongoDB. 

### 5. Сервис builder (wiki-builder)

**Назначение:** построение поискового индекса и артефактов (словарь терминов, postings, статистики и т.п.) на основе данных, доступных в MongoDB. 

### 6. Сервис searcher (wiki-search-engine)

**Назначение:** интерактивный поисковый сервис/CLI, который использует ранее построенный индекс в /exports.

### 7. Сервис `zipf_plot`

**Назначение:** построение графика/аналитики по закону Ципфа на основе экспортированных данных (частоты/словарь), размещённых в `/exports`.

---
# Запуск
___

1) Необходимо запустить сервис bootstrap, который в свою очередь поднимает сервер MySQL
```
make bootstrap-init
```
2) Необходимо запустить сервис crawler (поисковой робот), который в свою очередь поднимает сервер MongoDB
```
make crawl-init
```
3) Для проверки иницилизации MongoDB  и наличия в ней данных можно  запустить сервис verify_mongo (НЕОБЯЗАТЕЛЬНО)
```
make verify_mongo
```
4) Необходимо запустить сервис clean_html для очистки скаченных документов и подготовки их к дальнейшей токенизации и стемингу
```
make clean_html
```
5) Для построения булевого индекса (таблица postings.csv) и получения данных для Ципфа (таблица zipf.csv) необходимо запустить сервис builder 
```
make builder
```
6) Для построения графика распределения частот по Ципфу необходимо запустить сервис zipf
```
make zipf
```
7) Для запуска терминала булевого поиска по документам необходимо запустить сервис searcher
```
make searcher
```